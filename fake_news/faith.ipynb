{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection Using Logistic Regression and Decision Tree Classifiers\n",
    "## Fake news on different platforms is spreading widely and is a matter of serious concern, as it causes social wars and permanent breakage of the bonds established among people. A lot of research is already going on focused on the classification of fake news.\n",
    "### In this project, Decision Tree Classifier and Logistic regression are performing well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Look down the notebook for the results of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Import \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import dataset\n",
    "\n",
    "###  Result for the dataset import. The second line prints the first 10 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('News.csv',index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of the dataset\n",
    "\n",
    "### Result = (44919, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As the title, subject and date column is not going to be helpful in identification of the news. So, we can drop these column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data = data.drop([\"title\", \"subject\",\"date\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to check if there is any null value (we will drop those rows)\n",
    "\n",
    "## Result for the check\n",
    "###  text     0\n",
    "###  class    0 \n",
    "\n",
    "#### There are no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have to shuffle the dataset to prevent the model to get bias. After that we will reset the index and then drop it. Because index column is not useful to us.\n",
    "## Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1)\n",
    "data.reset_index(inplace=True)\n",
    "data.drop([\"index\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Let’s explore the unique values in the each category using below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.countplot(data=data,\n",
    "\t\t\tx='class',\n",
    "\t\t\torder=data['class'].value_counts().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and analysis of News column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firstly we will remove all the stopwords, punctuations and any irrelevant spaces from the text. For that NLTK Library is required and some of it’s module need to be downloaded. So, for that run the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we have all the required modules, we can create a function name preprocess text. This function will preprocess all the data given as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_data):\n",
    "\tpreprocessed_text = []\n",
    "\t\n",
    "\tfor sentence in tqdm(text_data):\n",
    "\t\tsentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "\t\tpreprocessed_text.append(' '.join(token.lower()\n",
    "\t\t\t\t\t\t\t\tfor token in str(sentence).split()\n",
    "\t\t\t\t\t\t\t\tif token not in stopwords.words('english')))\n",
    "\n",
    "\treturn preprocessed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  To implement the function in all the news in the text column, run the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_review = preprocess_text(data['text'].values)\n",
    "data['text'] = preprocessed_review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s visualize the WordCloud for fake and real news separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated = ' '.join(\n",
    "\tword for word in data['text'][data['class'] == 1].astype(str))\n",
    "wordCloud = WordCloud(width=1600,\n",
    "\t\t\t\t\theight=800,\n",
    "\t\t\t\t\trandom_state=21,\n",
    "\t\t\t\t\tmax_font_size=110,\n",
    "\t\t\t\t\tcollocations=False)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated = ' '.join(\n",
    "\tword for word in data['text'][data['class'] == 0].astype(str))\n",
    "wordCloud = WordCloud(width=1600,\n",
    "\t\t\t\t\theight=800,\n",
    "\t\t\t\t\trandom_state=21,\n",
    "\t\t\t\t\tmax_font_size=110,\n",
    "\t\t\t\t\tcollocations=False)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordCloud.generate(consolidated), interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, Let’s plot the bargraph of the top 20 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "\tvec = CountVectorizer().fit(corpus)\n",
    "\tbag_of_words = vec.transform(corpus)\n",
    "\tsum_words = bag_of_words.sum(axis=0)\n",
    "\twords_freq = [(word, sum_words[0, idx])\n",
    "\t\t\t\tfor word, idx in vec.vocabulary_.items()]\n",
    "\twords_freq = sorted(words_freq, key=lambda x: x[1],\n",
    "\t\t\t\t\t\treverse=True)\n",
    "\treturn words_freq[:n]\n",
    "\n",
    "\n",
    "common_words = get_top_n_words(data['text'], 20)\n",
    "df1 = pd.DataFrame(common_words, columns=['Review', 'count'])\n",
    "\n",
    "df1.groupby('Review').sum()['count'].sort_values(ascending=False).plot(\n",
    "\tkind='bar',\n",
    "\tfigsize=(10, 6),\n",
    "\txlabel=\"Top Words\",\n",
    "\tylabel=\"Count\",\n",
    "\ttitle=\"Bar Chart of Top Words Frequency\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text into Vectors\n",
    "### Before converting the data into vectors, split it into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['text'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tdata['class'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can convert the training data into vectors using TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorization = TfidfVectorizer()\n",
    "x_train = vectorization.fit_transform(x_train)\n",
    "x_test = vectorization.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training, Evaluation, and Prediction\n",
    "### Now, the dataset is ready to train the model.\n",
    "### For training we will use Logistic Regression and evaluate the prediction accuracy using accuracy_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# testing the model\n",
    "print(accuracy_score(y_train, model.predict(x_train)))\n",
    "print(accuracy_score(y_test, model.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let’s train with Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# testing the model\n",
    "print(accuracy_score(y_train, model.predict(x_train)))\n",
    "print(accuracy_score(y_test, model.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The confusion matrix for Decision Tree Classifier can be implemented with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of Results from Decision Tree classification\n",
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(y_test, model.predict(x_test))\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "\t\t\t\t\t\t\t\t\t\t\tdisplay_labels=[False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
